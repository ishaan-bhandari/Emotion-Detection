{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "nanct0VrFr5S"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "vTvlQpRbknCZ",
        "outputId": "c7282ff4-b6cc-47d0-f8ad-76c005c4f4ce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#@title Run this to prepare our environment\n",
        "\n",
        "\n",
        "# Imports the required libraries\n",
        "import cv2\n",
        "import dlib\n",
        "import math\n",
        "import gdown\n",
        "import unittest\n",
        "import numpy as np\n",
        "import urllib.request\n",
        "\n",
        "from scipy.spatial import distance\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "###Getting the Dlib Shape predictor!\n",
        "\n",
        "dlibshape_url = 'https://drive.google.com/uc?id=17D3D89Gke6i5nKOvmsbPslrGg5rVgOwg'\n",
        "dlibshape_path ='./shape_predictor_68_face_landmarks.dat'\n",
        "gdown.download(dlibshape_url, dlibshape_path, True)\n",
        "\n",
        "print (\"Done\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_u8_a0JelHpd"
      },
      "source": [
        "#Milestone 1: Understanding Face Detection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_rNLO05tEWA"
      },
      "source": [
        "##Face Detection Demonstration\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iB26IT8utRwi"
      },
      "source": [
        "Face Detection is an important step in the emotion classification pipeline. It helps us eliminate parts of the image which have no relevance in detecting the emotion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6KR5meZxv-nq"
      },
      "source": [
        "Face detection algorithms are used to predict the bounding box co-ordinates of the face\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1ZE-3eN2sarQ0h9kusTO5hosWLi2S4mlw)\n",
        "\n",
        "Dlib is a popular Python library complied in C++. For this project we will use\n",
        "Dlib's pre-trained Face detection model to extract the bounding box co-ordinates of the face\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OKcl354K4cAF"
      },
      "source": [
        "###Load Pretrained Dlib model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2-pHGj2OkRWJ"
      },
      "source": [
        "# Load's dlib's pretrained face detector model\n",
        "frontalface_detector = dlib.get_frontal_face_detector()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BQnR1UJVmW2r"
      },
      "source": [
        "#@title Run this cell to define a helper Function for Face Detection\n",
        "\n",
        "'''\n",
        "  Converts dlib rectangular object to bounding box co-ordinates\n",
        "'''\n",
        "def rect_to_bb(rect):\n",
        "    # take a bounding predicted by dlib and convert it\n",
        "    # to the format (x, y, w, h) as we would normally do\n",
        "    # with OpenCV\n",
        "    x = rect.left()\n",
        "    y = rect.top()\n",
        "    w = rect.right() - x\n",
        "    h = rect.bottom() - y\n",
        "    return (x, y, w, h)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vo7m32hHnE7u"
      },
      "source": [
        "#@title Run this cell to define a helper Function for Face Detection with a given image\n",
        "\n",
        "\"\"\"\n",
        "Detects the face in the given image\n",
        "\"\"\"\n",
        "def detect_face(image_url):\n",
        "  \"\"\"\n",
        "  :type image_url: str\n",
        "  :rtype: None\n",
        "\n",
        "  \"\"\"\n",
        "  try:\n",
        "\n",
        "    #Decodes image address to cv2 object\n",
        "    url_response = urllib.request.urlopen(image_url)\n",
        "    img_array = np.array(bytearray(url_response.read()), dtype=np.uint8)\n",
        "    image = cv2.imdecode(img_array, -1)\n",
        "\n",
        "  except Exception as e:\n",
        "    return \"Please check the URL and try again!\"\n",
        "\n",
        "  #Detect faces using dlib model\n",
        "  rects = frontalface_detector(image, 1)\n",
        "\n",
        "  if len(rects) < 1:\n",
        "    return \"No Face Detected\"\n",
        "\n",
        "  # Loop over the face detections\n",
        "  for (i, rect) in enumerate(rects):\n",
        "    # Converts dlib rectangular object to bounding box co-ordinates\n",
        "    (x, y, w, h) = rect_to_bb(rect)\n",
        "    cv2.rectangle(image, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
        "\n",
        "  plt.imshow(image, interpolation='nearest')\n",
        "  plt.axis('off')\n",
        "  plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "llTv-nv44YX4"
      },
      "source": [
        "### Face Detection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Erulyghe4T_a"
      },
      "source": [
        "###Try it Out!!!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1g5S3o_euWsQ"
      },
      "source": [
        "# https://www.clickinmoms.com/blog/wp-content/uploads/2014/10/black-and-white-portrait-of-man-with-his-eyes-closed-by-Brian-Powers.jpg\n",
        "# https://i.pinimg.com/736x/a8/59/05/a85905aad4b379aafd63bbbd3144025d--freya-mavor-beautiful-people.jpg\n",
        "# https://i.pinimg.com/236x/27/28/0e/27280ee28567c1e20c119f74981ee5c4--black-freckles-freckles-makeup.jpg\n",
        "\n",
        "# Give the path of the image for face detection\n",
        "detect_face(input('Enter the URL of the image: '));"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cD34pERNy6my"
      },
      "source": [
        "#Milestone 2: Understanding Facial Landmarks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_uaTewLrzINj"
      },
      "source": [
        "**What are Facial Landmarks?**\n",
        "\n",
        "\n",
        "Facial landmarks are a set of key points on human face images/Facial Landmarks represent the points of interest within the face. The points are defined by their (x,y) coordinates on the image. These points are used to locate and represent salient regions of the face, such as eyes, eyebrows, nose, mouth and jawline."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4V4NJXsN0jXS"
      },
      "source": [
        "##Facial Landmark Demonstration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "520Q_tTT0Zbh"
      },
      "source": [
        "Facial Landmark estimation is an important feature extraction steps in solving variety of applications such as face recognition, facial expression recognition, face swapping, face filters and much more."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BVkMvanE2tsk"
      },
      "source": [
        "The number of Facial key points on the face can be variable depending on the pre-trained facial landmark model being used.\n",
        "\n",
        "\n",
        "\n",
        "![](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQ3TMlMcOORwi88JUPO3xvHbjl8yBGDZnMMNhfpY5pS4Mvq_n7w)\n",
        "\n",
        "For this project, we will be using Dlib's pretrained Facial Landmark Detection Model which help us detect 68 2-Dimensional points on the human face\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZteCP8pz7KL"
      },
      "source": [
        "## Facial Landmark Estimation using DLib\n",
        "\n",
        "In this section, we are going to look at the code to extract and plot the 68 Facial Landmarks for the given image.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CB9fqreHQJtk"
      },
      "source": [
        "###Load Pre-trained DLib models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KhVSMc7I1sX_"
      },
      "source": [
        "# Load's dlib's pretrained face detector model\n",
        "frontalface_detector = dlib.get_frontal_face_detector()\n",
        "#Load the 68 face Landmark file\n",
        "landmark_predictor = dlib.shape_predictor('./shape_predictor_68_face_landmarks.dat')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zAI9BJyoPoyl"
      },
      "source": [
        "### Extracting Facial Landmarks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HdV7TDoz14QL"
      },
      "source": [
        "#@title Run this cell to define a helper function for Face Detection from a url\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Returns facial landmarks for the given input image path\n",
        "\"\"\"\n",
        "def get_landmarks(image_url):\n",
        "  \"\"\"\n",
        "  :type image_url : str\n",
        "  :rtype image : cv2 object\n",
        "  :rtype landmarks : list of tuples where each tuple represents\n",
        "                     the x and y co-ordinates of facial keypoints\n",
        "  \"\"\"\n",
        "\n",
        "  try:\n",
        "\n",
        "    #Decodes image address to cv2 object\n",
        "    url_response = urllib.request.urlopen(image_url)\n",
        "    img_array = np.array(bytearray(url_response.read()), dtype=np.uint8)\n",
        "    image = cv2.imdecode(img_array, -1)\n",
        "\n",
        "  except Exception as e:\n",
        "    print (\"Please check the URL and try again!\")\n",
        "    return None,None\n",
        "\n",
        "  #Detect the Faces within the image\n",
        "  faces = frontalface_detector(image, 1)\n",
        "  if len(faces):\n",
        "    landmarks = [(p.x, p.y) for p in landmark_predictor(image, faces[0]).parts()]\n",
        "  else:\n",
        "    return None,None\n",
        "\n",
        "  return image,landmarks"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "InE6rfBXPkEU"
      },
      "source": [
        "###Visualizing Facial Landmarks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l4npmNu0zgKz",
        "cellView": "form"
      },
      "source": [
        "#@title Run this cell to define a helper function to visualize landmarks\n",
        "\n",
        "\"\"\"\n",
        "Display image with its Facial Landmarks\n",
        "\"\"\"\n",
        "def image_landmarks(image,face_landmarks):\n",
        "  \"\"\"\n",
        "  :type image_path : str\n",
        "  :type face_landmarks : list of tuples where each tuple represents\n",
        "                     the x and y co-ordinates of facial keypoints\n",
        "  :rtype : None\n",
        "  \"\"\"\n",
        "  radius = -1\n",
        "  circle_thickness = 5\n",
        "  image_copy = image.copy()\n",
        "  for (x, y) in face_landmarks:\n",
        "    cv2.circle(image_copy, (x, y), circle_thickness, (255,0,0), radius)\n",
        "\n",
        "  plt.imshow(image_copy, interpolation='nearest')\n",
        "  plt.axis('off')\n",
        "  plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Vl6hSG4P6RJ"
      },
      "source": [
        "###Try it Out!!!!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YLeqLY0Xzk1D"
      },
      "source": [
        "#Extract the Facial Landmark co-ordinates\n",
        "image,landmarks= get_landmarks(input(\"Enter the URL of the image: \")) #url\n",
        "\n",
        "#Plot the Facial Landmarks on the face\n",
        "if landmarks:\n",
        "  image_landmarks(image,landmarks)\n",
        "else:\n",
        "  print (\"No Landmarks Detected\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IFKqtolfgouD",
        "cellView": "form"
      },
      "source": [
        "#@title Run (and eventually edit) this cell to visualize the features we've extracted\n",
        "\n",
        "def show_indices(landmarks, i_index):\n",
        "\n",
        "  plt.scatter(x=[landmarks[i][0] for i in range(len(landmarks)//2, len(landmarks))],\n",
        "              y=[-landmarks[i][1] for i in range(len(landmarks)//2, len(landmarks))], s=50, alpha=.5, color='blue', label='second half of indices')\n",
        "\n",
        "  plt.scatter(x=[landmarks[i][0] for i in range(len(landmarks)//2)],\n",
        "              y=[-landmarks[i][1] for i in range(len(landmarks)//2)], color='red', alpha=.5, label='first half of indices')\n",
        "\n",
        "  # what should X and Y be to visualize the feature at i_index?\n",
        "  #plt.scatter(x=X, y=-Y,\n",
        "  #            color='purple', s=100, marker='x', label='feature at index %d'%i_index)\n",
        "\n",
        "  plt.axis('off');\n",
        "  plt.legend(bbox_to_anchor=[1,1]);\n",
        "  plt.title('Visualizing the features we\\'ve extracted from this image',y =1.2);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8FX7o0LZgp6e"
      },
      "source": [
        "show_index = 30\n",
        "show_indices(landmarks, show_index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nanct0VrFr5S"
      },
      "source": [
        "## Exercise 2B (Discussion) | 10 Minutes | Within a student group\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DUjwuDg6JjRq"
      },
      "source": [
        "#@title Which Facial Landmark points correspond to which part of the face? { display-mode: \"form\" }\n",
        "LeftEye= \"36-41\" #@param[\"0-16\", \"17-26\", \"27-35\",\"36-41\",\"42-47\",\"48-67\",\"Fill Me In\"]\n",
        "RightEye = \"42-47\"#@param[\"0-16\", \"17-26\", \"27-35\",\"36-41\",\"42-47\",\"48-67\",\"Fill Me In\"]\n",
        "Eyebrows = \"17-26\"#@param[\"0-16\", \"17-26\", \"27-35\",\"36-41\",\"42-47\",\"48-67\",\"Fill Me In\"]\n",
        "Nose = \"27-35\"#@param[\"0-16\", \"17-26\", \"27-35\",\"36-41\",\"42-47\",\"48-67\",\"Fill Me In\"]\n",
        "Mouth = \"48-67\"#@param[\"0-16\", \"17-26\", \"27-35\",\"36-41\",\"42-47\",\"48-67\",\"Fill Me In\"]\n",
        "Jawline = \"0-16\"#@param[\"0-16\", \"17-26\", \"27-35\",\"36-41\",\"42-47\",\"48-67\",\"Fill Me In\"]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if LeftEye == \"36-41\":\n",
        "  print(\"The Left eye can be accessed through points %s\"%LeftEye)\n",
        "else:\n",
        "  print('Not quite %s'%LeftEye)\n",
        "\n",
        "if RightEye == \"42-47\":\n",
        "  print(\"The Right eye can be accessed through points %s\"%RightEye)\n",
        "else:\n",
        "  print('Not quite %s'%RightEye)\n",
        "\n",
        "if Eyebrows == \"17-26\":\n",
        "  print(\"The Eyebrows can be accessed through points %s\"%Eyebrows)\n",
        "else:\n",
        "  print('Not quite %s'%Eyebrows)\n",
        "\n",
        "if Nose == \"27-35\":\n",
        "  print(\"The Nose can be accessed through points %s\"%Nose)\n",
        "else:\n",
        "  print('Not quite %s'%Nose)\n",
        "\n",
        "if Mouth == \"48-67\":\n",
        "  print(\"The Mouth can be accessed through points %s\"%Mouth)\n",
        "else:\n",
        "  print('Not quite %s'%Mouth)\n",
        "\n",
        "if Jawline == \"0-16\":\n",
        "  print(\"The Jawline can be accessed through points %s\"%Jawline)\n",
        "else:\n",
        "  print('Not quite %s'%Jawline)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZBtFtUkN4RF"
      },
      "source": [
        "## Exercise 2C (Coding)\n",
        "\n",
        "In this section, you will modify the inputs to `image_landmarks`  function defined in previous section to detect and display different parts of the face individually using Facial landmarks.\n",
        "\n",
        "Write code to detect eyes, nose, mouth, jawline and eyebrows using Facial Landmarks.\n",
        "\n",
        "Hint: To detect the eyes, you need to plot Facial Landmark points from 36-47\n",
        "\n",
        "Note: Make sure you have valid facial landmark output after running the previous block (Try it Out Section!)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nDvFceG3N17i"
      },
      "source": [
        "# Display images with individual detection of face parts\n",
        "\n",
        "# For example, for eye detection\n",
        "\n",
        "eye_points = (36,47)\n",
        "selected_landmarks = landmarks[eye_points[0]:eye_points[1]+1]\n",
        "image_landmarks(image,selected_landmarks)\n",
        "\n",
        "### YOUR CODE HERE\n",
        "eyebrow_points = (17,26)\n",
        "selected_landmarks = landmarks[eyebrow_points[0]:eyebrow_points[1]+1]\n",
        "image_landmarks(image,selected_landmarks)\n",
        "\n",
        "jawline_points = (0,16)\n",
        "selected_landmarks = landmarks[jawline_points[0]:jawline_points[1]+1]\n",
        "image_landmarks(image,selected_landmarks)\n",
        "\n",
        "nose_points = (27,34)\n",
        "selected_landmarks = landmarks[nose_points[0]:nose_points[1]+1]\n",
        "image_landmarks(image,selected_landmarks)\n",
        "\n",
        "mouth_points = (48, 66)\n",
        "selected_landmarks = landmarks[mouth_points[0]:mouth_points[1]+1]\n",
        "image_landmarks(image,selected_landmarks)\n",
        "### END CODE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A355Uv7wDMDl"
      },
      "source": [
        "#Milestone 3: Understanding Euclidean Distances\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hbf8TZPw-A7r"
      },
      "source": [
        "Euclidean distance between between points p and q is is equal to the length of the line segment connecting them. When data is dense or continuous, this is the best proximity measure.\n",
        "\n",
        "In this section, we will explore how euclidean distance between pairs of Facial Landmarks can help solve simple use cases related to faces"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10B9RCvzC6y7"
      },
      "source": [
        "##Exercise 3A (Discussion) | 5 Minutes | Within a student group"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NockIqDUmCEM"
      },
      "source": [
        "![](https://drive.google.com/uc?export=view&id=1UTtJOQRh6ebj86SIPr-WfdRgDS2nb3m_)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KtogEi-XkmVP"
      },
      "source": [
        "###What is the difference between two images? Can you use facial landmarks  to distinguish between the two images?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AgWCy3ho-s2U"
      },
      "source": [
        "Take a look at the images with facial landmarks superimposed over them!\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=16O3KupWH090-9avnsah8v1LcM80okgmX)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_XQvxhwvSnYR"
      },
      "source": [
        "Does the distance between certain facial landmarks help us distinguish between two images?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g4awukiot2tu"
      },
      "source": [
        "##Exercise 3B (Coding) | Are the eyes open or close ?\n",
        "\n",
        "In last block, we figured out which set of landmarks help us distinguish between the two images. In this section, you will write code to distinguish between closed eyes and open eyes using facial Landmarks\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cmSPfiVZ-_7a"
      },
      "source": [
        "###Euclidean Distance\n",
        "\n",
        "Write a function to compute the euclidean distance between two points"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_IsP9gUpcun"
      },
      "source": [
        "\"\"\"\n",
        "Computes the euclidean distance between 2 points in 2D space\n",
        "#inexing#math.sqrt\n",
        "\"\"\"\n",
        "def euclidean_distance(p1,p2):\n",
        "  \"\"\"\n",
        "  type p1, p2 : tuple\n",
        "  rtype distance: float\n",
        "  \"\"\"\n",
        "  ### YOUR CODE HERE\n",
        "  return np.sqrt((p1[0]-p2[0])**2 + (p1[1] - p2[1])**2)\n",
        "  ### END CODE\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xw2ZuICTir0Q"
      },
      "source": [
        "### Are there other distance metrics we can use here?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQZnD_48_Dl8"
      },
      "source": [
        "###Classify images based on eyes\n",
        "\n",
        "Write code to find out which image corresponds to closed eyes and which image corresponds to open eyes using the concept of euclidean distance\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_7enTs1C7Jj8"
      },
      "source": [
        "###Psuedo-Algorithm\n",
        "\n",
        "1. Identity the Facial Landmarks of Interest\n",
        "2. Compute the distances between the points of interest\n",
        "3. Compare the distances of both the images\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0tHD4KL10Mn4"
      },
      "source": [
        "\"\"\"\n",
        "Distinguishes between two images--->closed eyes v/s open eyes\n",
        "\"\"\"\n",
        "def classify_images(image1_path,image2_path,plt_flag):\n",
        "  \"\"\"\n",
        "  type image1_path,image2_path: str\n",
        "  type plt_flag: boolean #Displays input images if True\n",
        "  rtype : str\n",
        "  \"\"\"\n",
        "  image1,image1_landmarks = get_landmarks(image1_path)\n",
        "  image2,image2_landmarks = get_landmarks(image2_path)\n",
        "\n",
        "  if plt_flag:\n",
        "    #Plot image1\n",
        "    plt.imshow(image1, interpolation='nearest')\n",
        "    plt.title(\"Image1\")\n",
        "    plt.show()\n",
        "\n",
        "    #Plot image2\n",
        "    plt.imshow(image2, interpolation='nearest')\n",
        "    plt.title(\"Image2\")\n",
        "    plt.show()\n",
        "  ### YOUR CODE HERE\n",
        "    pairs_distance = [(37, 41), (38, 40), (43, 47), (44, 46)]\n",
        "    d_sum1 = 0\n",
        "    d_sum2 = 0\n",
        "    for pair in pairs_distance:\n",
        "      d_sum1 = d_sum1 + euclidean_distance(image1_landmarks[pair[0]], image1_landmarks[pair[1]])\n",
        "      d_sum2 = d_sum2 + euclidean_distance(image2_landmarks[pair[0]], image2_landmarks[pair[1]])\n",
        "    print(d_sum1)\n",
        "    print(d_sum2)\n",
        "\n",
        "classify_images('https://i.pinimg.com/236x/27/28/0e/27280ee28567c1e20c119f74981ee5c4--black-freckles-freckles-makeup.jpg','https://www.clickinmoms.com/blog/wp-content/uploads/2014/10/black-and-white-portrait-of-man-with-his-eyes-closed-by-Brian-Powers.jpg', True )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCAZ73T2bsPS"
      },
      "source": [
        "### Exercise 3C (Discussion) | 5 Minutes | Within a student group"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fW0G6t-L-R0z"
      },
      "source": [
        "###Take a look at the images below! Do you think our logic would work for all the image sets?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "340Rt1Ii9vVS"
      },
      "source": [
        "![](https://drive.google.com/uc?export=view&id=1lpOYxFXKMCN_nafXmhGdLt3dvivx6gm8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YxZETngJefRU"
      },
      "source": [
        "##Optional Activity (Coding)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jHyaqS-Hel-L"
      },
      "source": [
        "Modify your code to distinguish between open and closed mouth using Facial Landmarks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uIq65g8Yez0u"
      },
      "source": [
        "### YOUR CODE HERE\n",
        "\n",
        "### END CODE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "csUQshl1fer7"
      },
      "source": [
        "#Milestone 4: Understanding Emotion Detection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pPUWsQkJr32l"
      },
      "source": [
        "##What distinguishes one emotion from another ?\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1rOn9Xc8-WZYuuE0IkLHhIR9pIvL0JJmx)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pe0Jf4HGsCoM"
      },
      "source": [
        "##Exercise (Discussion) | 10 Minutes | Within a student group"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FwyOqxKgsHz1"
      },
      "source": [
        "\n",
        "\n",
        "*   **What distinguishes Happy Face from Surprised Face?**\n",
        "*   **What distinguishes Happy Face from Neutral Face?**\n",
        "*   **What distinguishes Happy Face from Sad Face?**\n",
        "*   **What distinguishes Happy Face from Angry Face?**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "opCzMUPBFKzq"
      },
      "source": [
        "#Finish!"
      ]
    }
  ]
}